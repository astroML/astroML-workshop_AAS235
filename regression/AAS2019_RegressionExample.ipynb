{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measurement Errors in Linear Regression\n",
    "### January 6, 2020\n",
    "\n",
    "[Brigitta Sip≈ëcz, University of Washington](https://bsipocz.github.io/) \n",
    "\n",
    "Resources for this notebook:\n",
    "\n",
    "- [Textbook](http://press.princeton.edu/titles/10159.html) Chapter 8.\n",
    "- Paper by [B Kelly, ApJ, 665, 2007](https://iopscience.iop.org/article/10.1086/519947/fulltext/70991.html)\n",
    "- [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas\n",
    "\n",
    "We rely on scikit-learn, astroML and pymc3. Full functionality will be available in the next release of astroML (v1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets use simulated data here. \n",
    "First we will model the distance of 100 supernovas (for a particular cosmology) as a function of redshift.\n",
    "\n",
    "We rely on that astroML has a common API with scikit-learn, extending the functionality of the latter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.cosmology import LambdaCDM\n",
    "from astroML.datasets import generate_mu_z\n",
    "\n",
    "z_sample, mu_sample, dmu = generate_mu_z(100, random_state=0)\n",
    "\n",
    "cosmo = LambdaCDM(H0=70, Om0=0.30, Ode0=0.70, Tcmb0=0)\n",
    "z = np.linspace(0.01, 2, 1000)\n",
    "mu_true = cosmo.distmod(z)\n",
    "\n",
    "plt.errorbar(z_sample, mu_sample, dmu, fmt='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple linear regression\n",
    "\n",
    "Regression defined as the relation between a dependent variable, $y$, and a set of independent variables, $x$, \n",
    "that describes the expectation value of y given x: $ E[y|x] $\n",
    "\n",
    "We will start with the most familiar linear regression, a straight-line fit to data.\n",
    "A straight-line fit is a model of the form\n",
    "$$\n",
    "y = ax + b\n",
    "$$\n",
    "where $a$ is commonly known as the *slope*, and $b$ is commonly known as the *intercept*.\n",
    "\n",
    "We can use Scikit-Learn's LinearRegression estimator to fit this data and construct the best-fit line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression as LinearRegression_sk \n",
    "\n",
    "linear_sk = LinearRegression_sk()\n",
    "linear_sk.fit(z_sample[:,None], mu_sample)\n",
    "\n",
    "mu_fit_sk = linear_sk.predict(z[:, None])\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(z, mu_fit_sk, '-k')\n",
    "ax.plot(z, mu_true, '--', c='gray')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(36.01, 48)\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurement errors\n",
    "\n",
    "Modifications to LinearRegression in astroML take measurement errors into account on the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.linear_model import LinearRegression\n",
    "\n",
    "linear = LinearRegression()\n",
    "linear.fit(z_sample[:,None], mu_sample, dmu)\n",
    "\n",
    "mu_fit = linear.predict(z[:, None])\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(z, mu_fit_sk, '-k')\n",
    "ax.plot(z, mu_fit, '-k', color='red')\n",
    "\n",
    "ax.plot(z, mu_true, '--', c='gray')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(36.01, 48)\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basis function regression\n",
    "\n",
    "If we consider a function in terms of the sum of bases (this can be polynomials, Gaussians, quadratics, cubics) then we can solve for the coefficients using regression. \n",
    "\n",
    "### Polynomial basis functions\n",
    "\n",
    "polynomial regression: $$ùë¶=ùëé_0+ùëé_1ùë•+ùëé_2ùë•^2+ùëé_3ùë•^3+‚ãØ$$\n",
    "\n",
    "Notice that this is still a linear model‚Äîthe linearity refers to the fact that the coefficients $ùëé_ùëõ$ never multiply or divide each other. What we have effectively done here is to take our one-dimensional $ùë•$ values and projected them into a higher dimension, so that a linear fit can fit more complicated relationships between $ùë•$ and $ùë¶$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.linear_model import PolynomialRegression\n",
    "\n",
    "# 2nd degree polynomial regression\n",
    "polynomial = PolynomialRegression(degree=2)\n",
    "polynomial.fit(z_sample[:,None], mu_sample, dmu)\n",
    "\n",
    "mu_fit_poly = polynomial.predict(z[:, None])\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(z, mu_fit_poly, '-k', color='red')\n",
    "\n",
    "ax.plot(z, mu_true, '--', c='gray')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(36.01, 48)\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian basis functions\n",
    "\n",
    "Of course, other basis functions are possible.\n",
    "For example, one useful pattern is to fit a model that is not a sum of polynomial bases, but a sum of Gaussian bases. E.g. we could substitute $ùë•^2$ for Gaussians (where we fix $ùúé$ and $ùúá$ and fit for the amplitude) as long as the attribute we are fitting for is linear. This is called basis function regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.linear_model import BasisFunctionRegression\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define our number of Gaussians\n",
    "nGaussians = 20\n",
    "basis_mu = np.linspace(0, 2, nGaussians)[:, None]\n",
    "basis_sigma = 3 * (basis_mu[1] - basis_mu[0])\n",
    "\n",
    "gauss_basis = BasisFunctionRegression('gaussian', mu=basis_mu, sigma=basis_sigma)\n",
    "gauss_basis.fit(z_sample[:,None], mu_sample, dmu)\n",
    "\n",
    "mu_fit_gauss = gauss_basis.predict(z[:, None])\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(z, mu_fit_gauss, '-k', color='red')\n",
    "\n",
    "ax.plot(z, mu_true, '--', c='gray')\n",
    "ax.errorbar(z_sample, mu_sample, dmu, fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "ax.set_xlim(0.01, 1.8)\n",
    "ax.set_ylim(36.01, 48)\n",
    "\n",
    "ax.set_ylabel(r'$\\mu$')\n",
    "ax.set_xlabel(r'$z$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - how many polynomial terms (or Gaussians do we need to best fit the data)\n",
    "\n",
    "Increase the number of terms that we use to fit the data and plot the results until you are happy. Calculate the rms error (between the data `mu_sample` and the fit `mu_sample_fit`)\n",
    "\n",
    "Generate another set of input data and apply the model to these data. Is the rms the same? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization and cross-validation\n",
    "\n",
    "As the complexity of a model increases the data points fit the model more and more closely.\n",
    "\n",
    "This does not result in a better fit to the data. We are overfitting the data (the model has high variance - a small change in a training point can change the model dramatically).\n",
    "\n",
    "We can evaluate this using a training set (50-70% of sample), a cross-validation set (15-25%) and a test set (15-25%). See book sub-chapter 8.11 and e.g. figures [8.13](http://www.astroml.org/book_figures/chapter8/fig_cross_val_B.html) and [8.14](http://www.astroml.org/book_figures/chapter8/fig_cross_val_C.html).\n",
    "\n",
    "For cases where we are concerned with overfitting we can apply constraints (usually of smoothness, number of coefficients, size of coefficients). See book sub-chapter 8.3, and e.g. figure [8.4](http://www.astroml.org/book_figures/chapter8/fig_rbf_ridge_mu_z.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measurement errors in both dependent and independent variables\n",
    "\n",
    "Use simulation data from [Kelly 2007](https://iopscience.iop.org/article/10.1086/519947/pdf) where there is measurement error on the observed values $x_i$ and $y_i$ as well as intrinsic scatter in the regression relationship: $$ \\eta_i = \\alpha + \\beta \\xi_i + \\epsilon_i $$ and $$ x_i = \\xi_i + \\epsilon_{x,i}$$ $$y_i = \\eta_i + \\epsilon_{y,i}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulator_kelly import simulation\n",
    "\n",
    "ksi, eta, xi, yi, xi_error, yi_error, alpha_in, beta_in = simulation(size=100, scalex=0.2, scaley=0.2,\n",
    "                                                                     alpha=2, beta=[0.5,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearRegressionwithErrors will be part of the next astroML release, v1.0\n",
    "Change this import in the cell below to:\n",
    "\n",
    "`from astroML.linear_model import LinearRegressionwithErrors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_model import LinearRegressionwithErrors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_xy_err = LinearRegressionwithErrors()\n",
    "linreg_xy_err.fit(xi, yi, yi_error, xi_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some plotting functions we use below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_figure(ksi, eta, x, y, sigma_x, sigma_y, add_regression_lines=False,\n",
    "                alpha_in=1, beta_in=0.5):\n",
    "\n",
    "    figure = plt.figure(figsize=(15, 6))\n",
    "\n",
    "    ax = figure.add_subplot(121)\n",
    "    ax.scatter(x, y, alpha=0.5)\n",
    "    ax.errorbar(x, y, xerr=sigma_x, yerr=sigma_y, alpha=0.3, ls='')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "\n",
    "    # True regression line\n",
    "    x0 = np.linspace(-10, 10, 40)\n",
    "    y0 = alpha_in + x0 * beta_in\n",
    "\n",
    "    ax.plot(x0, y0, color='black', label='True')\n",
    "    ax.set_xlim(-12, 12)\n",
    "    ax.set_ylim(-5, 7)\n",
    "\n",
    "    ax.legend()\n",
    "    \n",
    "    \n",
    "def plot_trace(traces, observed, ax=None, chains=None, multidim_ind=None):\n",
    "\n",
    "    xi, yi, sigx, sigy = observed\n",
    "\n",
    "    if multidim_ind is not None:\n",
    "        xi = xi[multidim_ind]\n",
    "\n",
    "    x = np.linspace(np.min(xi)-0.5, np.max(xi)+0.5, 50)\n",
    "\n",
    "    for i, trace in enumerate(traces):\n",
    "        if 'theta' in trace.varnames and 'slope' not in trace.varnames:\n",
    "            trace.add_values({'slope': np.tan(trace['theta'])})\n",
    "\n",
    "        if multidim_ind is not None:\n",
    "            trace_slope = trace['slope'][:, multidim_ind]\n",
    "        else:\n",
    "            trace_slope = trace['slope'][:, 0]\n",
    "\n",
    "        if chains is not None:\n",
    "            for chain in range(100, len(trace) * trace.nchains, chains):\n",
    "                y = trace['inter'][chain] + trace_slope[chain] * x\n",
    "                ax.plot(x, y, alpha=0.03, c='red')\n",
    "\n",
    "        # plot the best-fit line only\n",
    "        H2D, bins1, bins2 = np.histogram2d(trace_slope,\n",
    "                                           trace['inter'], bins=50)\n",
    "\n",
    "        w = np.where(H2D == H2D.max())\n",
    "\n",
    "        # choose the maximum posterior slope and intercept\n",
    "        slope_best = bins1[w[0][0]]\n",
    "        intercept_best = bins2[w[1][0]]\n",
    "\n",
    "        print(\"beta:\", slope_best, \"alpha:\", intercept_best)\n",
    "        y = intercept_best + slope_best * x\n",
    "        \n",
    "        ax.plot(x, y, ':', label='fitted')\n",
    "        \n",
    "        ax.legend()\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_figure(ksi, eta, xi, yi, xi_error, yi_error, add_regression_lines=False, alpha_in=alpha_in, beta_in=beta_in)\n",
    "plot_trace([linreg_xy_err.trace,], (xi, yi, xi_error, yi_error), ax=plt.gca(), chains=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate regression\n",
    "For multivariate data (where we fit a hyperplane rather than a straight line) we simply extend the description of the regression function to multiple dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulator_kelly import simulation_multidim\n",
    "\n",
    "ksi, eta, xi, yi, xi_error, yi_error, alpha_in, beta_in = simulation_multidim(size=100, scalex=0.2, scaley=0.2,\n",
    "                                                                              alpha=2, beta=np.array((0.5, 1, 1)),\n",
    "                                                                              multidim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_xy_err = LinearRegressionwithErrors()\n",
    "linreg_xy_err.fit(xi, yi, yi_error, xi_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(linreg_xy_err.trace['slope'].shape[1]):\n",
    "\n",
    "    joinpl = sns.jointplot(linreg_xy_err.trace['slope'][:, i], linreg_xy_err.trace['inter'], kind='kde')\n",
    "    joinpl.ax_joint.plot(beta_in[i], alpha_in, 'x', color='red', ms=10)\n",
    "    joinpl.ax_marg_y.plot([0, 2], [alpha_in, alpha_in], color='red')\n",
    "    joinpl.ax_marg_x.plot([beta_in[i], beta_in[i]], [0, 2], color='red')\n",
    "    \n",
    "    plot_figure(ksi[i], eta, xi[i], yi, xi_error[i], yi_error, add_regression_lines=False, alpha_in=alpha_in, beta_in=beta_in[i])\n",
    "    plot_trace([linreg_xy_err.trace,], (xi, yi, xi_error, yi_error), ax=plt.gca(), chains=50, multidim_ind=i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
